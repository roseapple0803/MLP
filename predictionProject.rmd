---
title: "Prediction Assignment"
author: "Jennifer Yueh"
date: "Saturday, August 27, 2016"
output: html_document
---

#Executive Summary

Given a set of weight lifting exercise measures, recorded by wearable devices, the project is aiming to predict the 'classe' variable in the data set, which is associated with how well an activity was performed by wearers.


#Data Source

Both training dataset and testing dataset come from the Human Activity Recognition project. They were collected on 8 hours of activities of 4 healthy subjects, performing 5 classes of exercises: sitting-down, standing-up, standing, walking, and sitting.



```{r}
#library(RCurl)
library(ggplot2)
library(caret)
library(parallel)
library(doParallel)
```


```{r}
urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
pml <- read.csv(urlTrain)

urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testing <- read.csv(urlTest)
```


#Data Exploration and Cleaning


```{r}
toomanyNA <- function(x){
  sum(is.na(x)) / length(x) > 0.95
}


countNA <- function(x){
  sum(is.na(x))
}
```


```{r}
dim(pml)
str(pml)
summary(pml[, 2:6])
hist(pml$magnet_arm_x)
```

A quick exploration reveals: the dataset has 160 variables and 19622 observations. The scale of its variables very. Possibly a lot of them are not normally distributed. Some of the variables have many missing values.

The following steps are taken to make sure that the preditive model works well.

## (1) Remove zero and near zero variance predictors

This step is eliminate the predictors which only one unique or very few values before modelling starts. Their existance is likely to cause the model to crush. 

```{r, echo=FALSE}
nzv <- nearZeroVar(pml, saveMetrics = TRUE)
idxNzv <- which(nzv$nzv)
filteredPml <- pml[, -idxNzv]
```


## (2) Remove predictors containing mostly NAs

There are a lot of variables in which the values are 98% NAs. They don't contribute much to the response/outcome.

Also if there is missing data in the variables in caret package, it won't return a prediction.

```{r}
NAVec <- sapply(filteredPml, toomanyNA)
filteredPml <- filteredPml[, !NAVec]
```


## (3) imputing

Missing data, should be imputed as Prediction algorithms are built not to handle them in most cases.

```{r}
NAVec <- sapply(filteredPml, countNA)
NAVec[NAVec>0]
```

The result shows that none of the remaining variables have NA values.


## (4)Remove predictors irrelevant to the outcome variable

A few of the variables, such as user names and time stamps, are not relevant to the response/outcome


```{r}
filteredPml <- filteredPml[, -(1:6)]
```


## (5)Remove highly correlated variables

Some predictive models may benefit from reducing the level of correlation between the predictors. Also these highly correlated variables might cause overfitting. 


```{r}
descrCor <- cor(filteredPml[, -53])

highlyCorDescr <- findCorrelation(descrCor, cutoff = 0.5)

names(filteredPml)[highlyCorDescr]

filteredPml <- filteredPml[, -highlyCorDescr]

classeIdx <- ncol(filteredPml)
```


#Modeling Process


## (1)Data Splitting

```{r}
set.seed(3456)
inTrain <- createDataPartition(filteredPml$classe, p=0.7, list=FALSE, times=1)
trainSet <- filteredPml[inTrain, ]
validSet <- filteredPml[-inTrain, ]
```



## (2) Data Standarization

As mentioned earlier in data exploration, a lot of variables are in different scales or very skewed.


```{r}
preObj <- preProcess(trainSet[,-classeIdx], method = c("center", "scale"))
trainTransformed <- predict(preObj, trainSet[,-classeIdx])
trainSet[,-classeIdx] <- trainTransformed

validTransformed <- predict(preObj, validSet[,-classeIdx])
validSet[,-classeIdx] <- validTransformed

```


## (3) Data resampling and model fitting

Instead of using the default resampling method, bootstrapping, 10-fold cross-validation is applied here. Also by using allowParallel,  accuracy analysis can be done across a series of training runs. 

Random forests algorithm is used here as it is one of the best among classification algorithms - able to classify large amounts of data with accuracy.


```{r}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(33833)
fitControl <- trainControl(method='cv', number=10, allowParallel = TRUE)
modFit <- train(classe ~ ., data=trainSet, method='rf', trControl=fitControl)

stopCluster(cluster)
```


#Model performance


```{r}
preds <- predict(modFit, newdata=validSet)

M <- confusionMatrix(preds, validSet$classe)
M$overall
accuracy <- M$overall[1]
```

Since accuracy is 0.9835174172, out of sample error rate is = 1 - 0.9835174172 = 0.016


#Prediction on Testing Set

```{r}

# I forgot to go through the data transformation process before trying to fit the model on the new data set, 
# i.e., I should have standarized the data as I did for the training data set

# Once this fixed, the result from preTest is now correct.

#####################################################################
 testTransformed <- predict(preObj, testing[, -160])
 testing[, -160] <- testTransformed
#####################################################################
 
 predTest <- predict(modFit,newdata=testing)
 predTest


```

